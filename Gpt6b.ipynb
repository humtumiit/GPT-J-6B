{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d98c85",
   "metadata": {},
   "source": [
    "GPT-J-6B Inference DemoÂ¶\n",
    "\n",
    "This notebook demonstrates how to run the GPT-J-6B model. See the link for more details about the model, including evaluation metrics and credits.\n",
    "\n",
    "Install Dependencies\n",
    "First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).\n",
    "\n",
    "!!! Make sure you are using a TPU runtime! !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea34c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
      "Requirement already satisfied: jax[tpu]>=0.2.16 in /opt/conda/lib/python3.7/site-packages (0.2.16)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax[tpu]>=0.2.16) (3.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from jax[tpu]>=0.2.16) (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from jax[tpu]>=0.2.16) (1.19.5)\n",
      "Requirement already satisfied: libtpu-nightly==0.1.dev20210615 in /opt/conda/lib/python3.7/site-packages (from jax[tpu]>=0.2.16) (0.1.dev20210615)\n",
      "Requirement already satisfied: jaxlib==0.1.68 in /opt/conda/lib/python3.7/site-packages (from jax[tpu]>=0.2.16) (0.1.68)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.1.68->jax[tpu]>=0.2.16) (1.12)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.1.68->jax[tpu]>=0.2.16) (1.6.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->jax[tpu]>=0.2.16) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "!apt install zstd\n",
    "print(2)\n",
    "# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\n",
    "!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
    "print(3)\n",
    "!time tar -I zstd -xf step_383500_slim.tar.zstd\n",
    "print(4)\n",
    "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
    "print(5)\n",
    "!pip install -r mesh-transformer-jax/requirements.txt\n",
    "print(6)\n",
    "# jax 0.2.12 is required due to a regression with xmap in 0.2.13\n",
    "!pip install mesh-transformer-jax/ jax==0.2.12\n",
    "print(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdcee7",
   "metadata": {},
   "source": [
    "Execute the below cell when you are executing this code on Google CoLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e169bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import requests \n",
    "#from jax.config import config\n",
    "\n",
    "#colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
    "#url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
    "#requests.post(url)\n",
    "\n",
    "# The following is required to use TPU Driver as JAX's backend.\n",
    "#config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "#config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94036a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optax\n",
    "!pip install transformers\n",
    "print('optax and transformers installed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c8bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of devices are: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"number of devices are:\", jax.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e607d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2, dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "\n",
    "from jax.experimental import maps\n",
    "import numpy as np\n",
    "import optax\n",
    "import transformers\n",
    "\n",
    "from mesh_transformer.checkpoint import read_ckpt\n",
    "from mesh_transformer.sampling import nucleaus_sample\n",
    "from mesh_transformer.transformer_shard import CausalTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"layers\": 28,\n",
    "  \"d_model\": 4096,\n",
    "  \"n_heads\": 16,\n",
    "  \"n_vocab\": 50400,\n",
    "  \"norm\": \"layernorm\",\n",
    "  \"pe\": \"rotary\",\n",
    "  \"pe_rotary_dims\": 64,\n",
    "\n",
    "  \"seq\": 2048,\n",
    "  \"cores_per_replica\": 8,\n",
    "  \"per_replica_batch\": 1,\n",
    "}\n",
    "\n",
    "per_replica_batch = params[\"per_replica_batch\"]\n",
    "cores_per_replica = params[\"cores_per_replica\"]\n",
    "seq = params[\"seq\"]\n",
    "\n",
    "\n",
    "params[\"sampler\"] = nucleaus_sample\n",
    "\n",
    "# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n",
    "params[\"optimizer\"] = optax.scale(0)\n",
    "\n",
    "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
    "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
    "\n",
    "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
    "\n",
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "print('Created Tokenizer successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76438b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
    "\n",
    "network = CausalTransformer(params)\n",
    "\n",
    "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
    "\n",
    "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1376f",
   "metadata": {},
   "source": [
    "Run Model\n",
    "\n",
    "Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\n",
    "\n",
    "Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed).\n",
    "\n",
    "You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\n",
    "\n",
    "Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77222b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584238ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(context, top_p=0.9, temp=1.0, gen_len=512):\n",
    "    tokens = tokenizer.encode(context)\n",
    "\n",
    "    provided_ctx = len(tokens)\n",
    "    pad_amount = seq - provided_ctx\n",
    "\n",
    "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
    "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
    "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
    "\n",
    "    start = time.time()\n",
    "    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
    "\n",
    "    samples = []\n",
    "    decoded_tokens = output[1][0]\n",
    "\n",
    "    for o in decoded_tokens[:, :, 0]:\n",
    "      samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n",
    "\n",
    "    print(f\"completion done in {time.time() - start:06}s\")\n",
    "    return samples\n",
    "\n",
    "print(infer(\"EleutherAI is\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de004775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { form-width: \"300px\" }\n",
    "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "context = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\"\"\n",
    "\n",
    "print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
